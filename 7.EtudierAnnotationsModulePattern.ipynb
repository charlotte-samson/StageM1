{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pattern.fr import conjugate, lemma, parse, parsetree,pprint,pluralize, singularize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import spacy\n",
    "import hunspell\n",
    "import fr_core_news_sm\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charge le modèle de langue\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "h = hunspell.HunSpell('/usr/share/hunspell/fr.dic', '/usr/share/hunspell/fr.aff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[\"celui\",\"celle\",\"ceux\",',','\"','(',')',\"s'\",\"n'\",\"j'\",\"l'\",'l’','leurs','toutes',\"c'\",'dès','etc','y','avoir','lesquelles','chacun','oui','non','ou','ne','par','plus','moins','tout','tous','faire','qui','que','pour','dans','sur','faut','il','de','des','les','la','le','du','à','et','on','un','une','au','aux','son','sa','en','avec','pas','']\n",
    "#ajoute à la liste des stopwords ceux trouvés sur ce site\n",
    "#https://www.ranks.nl/stopwords/french\n",
    "stop=open('stopwords.txt', 'r',encoding='utf-8')\n",
    "for line in stop :\n",
    "    line=line.strip('\\ufeff')\n",
    "    stopwords.append(line.strip('\\n'))\n",
    "stop.close()\n",
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie liste de tuples (mot,tag)\n",
    "#query = parse(txt)\n",
    "def convert_tag_format(query): \n",
    "    word = query.split(' ')\n",
    "    postag = [(x.split('/')[0], x.split('/')[1]) for x in word]\n",
    "    return postag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie liste de tuples (mot,chunk, chunkbis)\n",
    "def convert_chunk_format(query): \n",
    "    word = query.split(' ')\n",
    "    postag = [(x.split('/')[0], x.split('/')[2],x.split('/')[3]) for x in word]\n",
    "    return postag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie liste de tuples (mot,tag) + phrase taggée brut (ex : Il/PRP/B-NP/O faut/VB/B-VP/O)\n",
    "#mot/tag/chunk/O tag (= outside) means that the word is not part of a chunk.\n",
    "def get_pos_tags(text): \n",
    "    tagged_sent = parse(text)\n",
    "    return convert_tag_format(tagged_sent), tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(word):\n",
    "    word = word.lower()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditions for acceptable word: length, stopword\n",
    "def acceptable_word(word):\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords and lemma(word.lower()) not in stopwords)\n",
    "    return accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return ex : {'NP': ['il', 'les abattoirs'], 'VP': ['faut fermer']}\n",
    "#extract entity from BIO encoding \n",
    "def extract_entity(filetext):\n",
    "    last_entity = '' \n",
    "    last_tag = '' \n",
    "    mention2entities = {} \n",
    "    for line in filetext.split('\\n'): \n",
    "        line = line.strip() \n",
    "        if line == '': \n",
    "            continue\n",
    "        line_split = line.split('\\t')\n",
    "        if re.search('B-', line_split[1]): \n",
    "            if last_entity != '': \n",
    "                if not last_tag in mention2entities:\n",
    "                    mention2entities[last_tag] = [] \n",
    "                mention2entities[last_tag].append(last_entity.strip())\n",
    "            last_entity = line_split[0] + ' '\n",
    "            last_tag = line_split[1][2:] \n",
    "        elif re.search('I-', line_split[1]): \n",
    "            last_entity += line_split[0] + ' '\n",
    "    if last_entity != '': \n",
    "        if not last_tag in mention2entities:\n",
    "            mention2entities[last_tag] = [] \n",
    "        mention2entities[last_tag].append(last_entity.strip())\n",
    "    return mention2entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase2consider=liste (ex : ['NP', 'ADJP'])\n",
    "#tagged_sent = phrase taggée brut (ex : Il/PRP/B-NP/O faut/VB/B-VP/O)\n",
    "#mention2entities = dico (ex : {'NP': ['il', 'les abattoirs'], 'VP': ['faut fermer']})\n",
    "\n",
    "#renvoie les entités présentes pour des types de phrase/groupe donnés, ex VP et NP etc\n",
    "def get_entities_from_phrase(tagged_sent, phrase2consider): \n",
    "    word = tagged_sent.split(' ')\n",
    "    bio_tags = [normalise(x.split('/')[0])+ '\\t'+ x.split('/')[2] for x in word]\n",
    "    bio_text = '\\n'.join(bio_tags)\n",
    "    mention2entities = extract_entity(bio_text)\n",
    "    \n",
    "    ## strip off unacceptable words \n",
    "    _mention2entities = {} \n",
    "    for mention in mention2entities: \n",
    "        if not mention in phrase2consider: \n",
    "            continue\n",
    "        _mention2entities[mention] = [] \n",
    "        for entity in mention2entities[mention]: \n",
    "            _entity = ' '.join([word for word in entity.split(' ') if acceptable_word(word)]).strip()\n",
    "            if _entity != '': \n",
    "                _mention2entities[mention].append(_entity)\n",
    "    entities = []\n",
    "    for mention in _mention2entities: \n",
    "        entities.extend(_mention2entities[mention])\n",
    "    #print(_mention2entities)\n",
    "    #print(mention2entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renvoie liste de keywords, ici considéré par défaut comme entités dans NP et ADJP\n",
    "def get_keywords(text, phrase2consider=['NP', 'ADJP']): \n",
    "    toRemove=[]\n",
    "    try:\n",
    "        postoks, tagged_sent = get_pos_tags(text)\n",
    "        entities = get_entities_from_phrase(tagged_sent, phrase2consider)\n",
    "        doc=nlp(text)\n",
    "        if (phrase2consider==['NP', 'ADJP']) :\n",
    "            for i in range(2,len(doc)) :\n",
    "                tok=doc[i]\n",
    "                beforetok=(doc[i-1])\n",
    "                if i>2 : \n",
    "                    before2tok=(doc[i-2])\n",
    "                if(tok.dep_=='nmod' and beforetok.dep_=='case' and tok.pos_=='NOUN' and tok.head.pos_=='NOUN'):\n",
    "                    add=tok.head.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    ##toRemove.append(tok.head.text)\n",
    "                    entities.append(tok.head.text)\n",
    "                    entities.append(tok.text)\n",
    "                if(tok.dep_=='nmod' and before2tok.dep_=='case' and tok.pos_=='NOUN' and tok.head.pos_=='NOUN' and beforetok.pos_=='ADJ'):\n",
    "                    add=tok.head.text+' '+before2tok.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    ##toRemove.append(tok.head.text)\n",
    "                    entities.append(tok.text)\n",
    "                    entities.append(tok.head.text)\n",
    "                if(tok.dep_=='appos' and tok.head.pos_=='NOUN') :\n",
    "                    add=tok.head.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    ##toRemove.append(tok.head.text)\n",
    "                    entities.append(tok.text)\n",
    "                    entities.append(tok.head.text)\n",
    "                if(tok.pos_=='NOUN' and tok.dep_=='fixed' and beforetok.pos_=='ADP' and beforetok.dep_=='advmod' and beforetok.head.pos_=='NOUN') :\n",
    "                    add=before2tok.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    ##toRemove.append(before2tok.text)\n",
    "                    entities.append(tok.text)\n",
    "                    entities.append(before2tok.text)\n",
    "                if(tok.pos_=='NUM' and before2tok.pos_=='NOUN' and beforetok.pos_=='ADP' and beforetok.dep_=='case' and beforetok.head==tok):\n",
    "                    add=before2tok.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    entities.append(tok.text)\n",
    "                if(tok.pos_=='NOUN' and beforetok.pos_=='DET' and before2tok.pos_=='NOUN') :\n",
    "                    add=before2tok.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                    entities.append(tok.text)\n",
    "                    entities.append(before2tok.text)\n",
    "                if( tok.text not in stopwords and tok.pos_=='ADJ' and beforetok.pos_=='ADJ' and before2tok.pos_=='NOUN' and tok.head==before2tok and beforetok.head==before2tok and tok.dep_=='amod' and beforetok.dep_=='amod') : \n",
    "                    add=before2tok.text+' '+beforetok.text+' '+tok.text\n",
    "                    entities.append(add)\n",
    "                #(tok.head.pos_=='NOUN' and tok.dep_=='amod') or (tok.pos_=='NOUN' and tok.dep_=='amod')\n",
    "                if(tok.head.pos_=='NOUN' and tok.dep_=='amod' and tok.text not in stopwords ) :\n",
    "                    if(tok.i < tok.head.i) :\n",
    "                        toAdd=tok.text+' '+tok.head.text\n",
    "                    else :\n",
    "                        toAdd=tok.head.text+' '+tok.text\n",
    "                    if(toAdd not in entities) :\n",
    "                        entities.append(toAdd)\n",
    "                    ##toRemove.append(tok.text)\n",
    "                    ##toRemove.append(tok.head.text)\n",
    "                    entities.append(tok.text)\n",
    "                    entities.append(tok.head.text)\n",
    "            for elt in toRemove :\n",
    "                if elt in entities :\n",
    "                    entities.remove(elt)\n",
    "    except: \n",
    "        return []\n",
    "    return list(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fc51d353f28>, {})\n"
     ]
    }
   ],
   "source": [
    "#modifier ce que retourne get_keywords\n",
    "entities=get_keywords(\"Il faut que les produits locaux soient au même prix que les produits importés, voire moins chers, pour qu'ils soient accessibles à tous\",['NP', 'ADJP'])\n",
    "lemme2words=defaultdict(lambda:defaultdict(int))\n",
    "keywords=[]\n",
    "for entity in entities : \n",
    "    for word in entity.split():\n",
    "        lemme=h.stem(word)[0].decode('utf-8')\n",
    "        lemme2words[lemme][word]+=1\n",
    "        keywords.append(lemme)\n",
    "print(lemme2words)\n",
    "for i in range(len(keywords)) : \n",
    "    keywords[i]=max(dict(lemme2words[keywords[i]]).items(), key=operator.itemgetter(1))[0]\n",
    "    print(keywords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom du fichier où se trouve le dico d'annotations\n",
      "json_annots\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'json_annots.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0e9505d6521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nom du fichier où se trouve le dico d'annotations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mjsontxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mjsondicoannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsontxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'json_annots.json'"
     ]
    }
   ],
   "source": [
    "#Si on effectue à partir d'un dico enregistré (json)\n",
    "print(\"Nom du fichier où se trouve le dico d'annotations\")\n",
    "filename=input()\n",
    "json_file = open(filename+'.json',encoding='utf-8')\n",
    "jsontxt = json_file.read()\n",
    "jsondicoannot=json.loads(jsontxt)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jsondicoannot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a50868b1ecec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mannotslemme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlemme2words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjsondicoannot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mkeywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mannotspattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mget_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jsondicoannot' is not defined"
     ]
    }
   ],
   "source": [
    "annotspattern=defaultdict(list)\n",
    "annotslemme=defaultdict(list)\n",
    "lemme2words=defaultdict(lambda:defaultdict(int))\n",
    "for k,v in jsondicoannot.items() :\n",
    "    keywords=[]\n",
    "    annotspattern[k]+=get_keywords(k,['NP', 'ADJP'])\n",
    "    for entity in annotspattern[k] : \n",
    "        lemme=\"\"\n",
    "        for word in entity.split():\n",
    "            try :\n",
    "                lemme=lemme+h.stem(word)[0].decode('utf-8')+\" \"\n",
    "            except IndexError :\n",
    "                pass;\n",
    "        lemme2words[lemme.strip()][entity]+=1\n",
    "        keywords.append(lemme.strip())\n",
    "    print(\"ANNOTLEMME\")\n",
    "    annotslemme[k]=keywords\n",
    "    print(annotslemme[k])\n",
    "    print(k)\n",
    "    print(\"pattern :\")\n",
    "    print(annotspattern[k])\n",
    "\"\"\"for i in range(len(keywords)) : \n",
    "    keywords[i]=max(dict(lemme2words[keywords[i]]).items(), key=operator.itemgetter(1))[0]\"\"\"\n",
    "for k,v in jsondicoannot.items() :\n",
    "    for i in range(len(annotspattern[k])) :\n",
    "        annotspattern[k][i]=max(dict(lemme2words[annotslemme[k][i]]).items(), key=operator.itemgetter(1))[0]\n",
    "    print(k)\n",
    "    print(\"pattern :\")\n",
    "    print(annotspattern[k])\n",
    "    \"\"\"print(k)\n",
    "    print(\"pattern :\")\n",
    "    print(annotspattern[k])\n",
    "    print(\"annots manuelles :\")\n",
    "    print(jsondicoannot[k])\n",
    "    print(\"communs : \")\n",
    "    print(set(annotspattern[k]).intersection(set(jsondicoannot[k])))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il faut aider à développer une agriculture raisonnée et à favoriser un élevage de qualité et sain\n",
      "['élevage de qualité', 'agriculture raisonnée']\n",
      "['élevage', 'sain', 'qualité', 'agriculture raisonnée']\n",
      "\n",
      "Rappel : 1/2\n",
      "Précision : 1/4\n",
      "Il faut faire revenir les petits commerçants au cœur des villes et villages et privilégier les circuits courts et le bio\n",
      "['petits commerçants', 'circuits courts', 'villes et villages', 'bio']\n",
      "['petits commerçants', 'villes villages', 'cœur', 'circuits courts', 'bio']\n",
      "\n",
      "Rappel : 3/4\n",
      "Précision : 3/5\n",
      "Il faut développer la vente en vrac \n",
      "['vente en vrac']\n",
      "['vrac', 'vente']\n",
      "\n",
      "Rappel : 0/1\n",
      "Précision : 0/2\n",
      "Il faut manger et créer français\n",
      "['français']\n",
      "['français']\n",
      "\n",
      "Rappel : 1/1\n",
      "Précision : 1/1\n",
      "Il faut continuer la suppression des substances controversées / additifs dans tous les produits alimentaires\n",
      "['additifs', 'suppression des substances controversées', 'produits alimentaires']\n",
      "['suppression', 'substances', 'produits alimentaires', 'additifs']\n",
      "\n",
      "Rappel : 2/3\n",
      "Précision : 2/4\n",
      "Il faut proposer comme en Allemagne des machines à consigner les bouteilles\n",
      "['machines à consigner les bouteilles', 'consigner', 'bouteilles', 'allemagne']\n",
      "['bouteilles', 'machines', 'allemagne']\n",
      "\n",
      "Rappel : 2/4\n",
      "Précision : 2/3\n",
      "Il faut s'inscrire dans les AMAP\n",
      "['AMAP']\n",
      "['allemagne']\n",
      "\n",
      "Rappel : 0/1\n",
      "Précision : 0/1\n",
      "Il faut augmenter les revenus pour que les gens puissent consommer\n",
      "['augmenter les revenus', 'consommer']\n",
      "['revenus', 'gens']\n",
      "\n",
      "Rappel : 0/2\n",
      "Précision : 0/2\n",
      "Il faut redistribuer nos subventions pour que les produits les moins chers ne soient pas les plus nocifs pour la santé et la planète \n",
      "['produits les moins chers', 'nocifs pour la santé', 'planète', 'redistribuer nos subventions']\n",
      "['nocifs', 'santé', 'redistribuer nos subventions', 'produits', 'chers', 'planète']\n",
      "\n",
      "Rappel : 2/4\n",
      "Précision : 2/6\n",
      "Il faut que les municipalités créent des régies agricoles bio et locales afin d'alimenter les cantines et les EHPAD en créant des emplois\n",
      "['ehpad', 'municipalités', 'locales', 'alimenter les cantines', 'régies agricoles bio']\n",
      "['municipalités', 'locales', 'cantines', 'emplois', 'régies agricoles bio', 'allemagne']\n",
      "\n",
      "Rappel : 3/5\n",
      "Précision : 3/6\n",
      "Il faut éduquer les enfants dès le plus jeune âge à cuisiner\n",
      "['éduquer', 'cuisiner', 'enfants']\n",
      "['jeune âge', 'enfants']\n",
      "\n",
      "Rappel : 1/3\n",
      "Précision : 1/2\n",
      "Il faut apprendre à consommer moins de viande, l'humain n'est pas un carnivore \n",
      "['carnivore', 'moins de viande']\n",
      "['carnivore', 'humain', 'viande']\n",
      "\n",
      "Rappel : 1/2\n",
      "Précision : 1/3\n",
      "Il faut accepter l'importation de légumes, mais avec un contrôle régulé par la Commission Européenne\n",
      "['contrôle régulé', 'commission européenne', 'importation de légumes', 'comission européenne']\n",
      "['importation', 'contrôle régulé', 'légumes', 'commission européenne']\n",
      "\n",
      "Rappel : 2/4\n",
      "Précision : 2/4\n",
      "Il faut manger des produits de saison et qui viennent de circuit court\n",
      "['produits de saison', 'circuit court']\n",
      "['saison', 'produits', 'circuits courts']\n",
      "\n",
      "Rappel : 0/2\n",
      "Précision : 0/3\n",
      "0.391304347826087\n",
      "0.47368421052631576\n"
     ]
    }
   ],
   "source": [
    "TP=0\n",
    "rapdenum=0\n",
    "precisiondenum=0\n",
    "for k,v in jsondicoannot.items() :\n",
    "    print(k)\n",
    "    print(list(set(jsondicoannot[k])))\n",
    "    print(list(set(annotspattern[k])))\n",
    "    print()\n",
    "    print(\"Rappel : \"+str(len(set(annotspattern[k]).intersection(set(jsondicoannot[k]))))+'/'+str(len(jsondicoannot[k])))\n",
    "    TP+=len(set(annotspattern[k]).intersection(set(jsondicoannot[k])))\n",
    "    rapdenum+=len(jsondicoannot[k])\n",
    "    print(\"Précision : \"+str(len(set(annotspattern[k]).intersection(set(jsondicoannot[k]))))+'/'+str(len(annotspattern[k])))\n",
    "    precisiondenum+=len(annotspattern[k])\n",
    "print(TP/precisiondenum)\n",
    "print(TP/rapdenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il faut une information claire concernant la qualité, l'origine des produits et surtout la consommation des produits de saison\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut prendre du temps pour préparer ses repas, ne jamais acheter du tout fait.\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut acheter directement au producteur (agriculteur) pour augmenter son revenu mensuel\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il ne faut plus d'élevages industriels, mais des élevages ou les animaux sont traités humainement\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut une taxe écologique pour les produits qui génèrent trop d'effets de serre, de pollution des sols, de l'eau etc.\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut apprendre à distinguer les calories des composants sains (glucides, protéines …) et les calories des sucres, graisses saturées, etc\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut beaucoup plus de produits BIO, notamment légumes et fruits frais dans les magasin U\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut que les produits locaux soient au même prix que les produits importés, voire moins chers, pour qu'ils soient accessibles à tous\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut que les agriculteurs vendent plus directement leurs produits aux consommateurs, en évitant les supermarchés qui bétonisent les sols.\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut une interdiction nationale des pesticides \n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut imposer les prix aux commerçants pour que l'on puisse acheter plus\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut que les gens loin des marchés publics ou des paysans puissent aussi avoir accès aux produits de saison\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut bannir la vente de viande issue d'élevages intensifs qui nuisent à l'environnement, à la santé et provoque la souffrance animale \n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut revoir le système des supermarchés qui font beaucoup trop d'offres et pas vraiment de la qualité. Il y a trop de gaspillage\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n",
      "Il faut arrêter définitivement les aliments aux risques endocriniens\n",
      "pattern :\n",
      "set()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#keywords uniquement VP\n",
    "for k,v in jsondicoannot.items() :\n",
    "    print(k)\n",
    "    print(\"pattern :\")\n",
    "    print(set(get_keywords(k,['VP'])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP [('Il', 'PRP')] None\n",
      "VP [('faut', 'VB'), ('manger', 'VB')] None\n",
      "PP [('des', 'IN')] des produits\n",
      "NP [('produits', 'NNS')] des produits\n",
      "PP [('de', 'IN')] de saison\n",
      "NP [('saison', 'NN')] de saison\n",
      "VP [('viennent', 'VB')] None\n",
      "PP [('de', 'IN')] de circuit court\n",
      "NP [('circuit', 'NN'), ('court', 'RB')] de circuit court\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(\"Il faut manger des produits de saison et qui viennent de circuit court.\", relations=True, lemmata=True)\n",
    "for sentence in s : \n",
    "    for chunk in sentence.chunks : \n",
    "        print(chunk.type, [(w.string, w.type) for w in chunk.words], chunk.pnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(parse('Il faut manger des produits de saison et qui viennent de circuit court', relations=True, lemmata=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP [('Il', 'PRP')] None\n",
      "VP [('faut', 'VB'), ('éviter', 'VB')] None\n",
      "PP [(\"d'\", 'IN')] None\n",
      "VP [('acheter', 'VB')] None\n",
      "PP [('des', 'IN')] des produits transformés\n",
      "NP [('produits', 'NNS')] des produits transformés\n",
      "VP [('transformés', 'VBN')] des produits transformés\n",
      "VP [('cuisiner', 'VB')] None\n",
      "PP [('avec', 'IN'), ('des', 'IN')] avec des produits\n",
      "NP [('produits', 'NNS')] avec des produits\n",
      "PP [('de', 'IN')] de base\n",
      "NP [('base', 'NN')] de base\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(\"Il faut éviter d'acheter des produits transformés et cuisiner avec des produits de base\", relations=True, lemmata=True)\n",
    "for sentence in s : \n",
    "    for chunk in sentence.chunks : \n",
    "        print(chunk.type, [(w.string, w.type) for w in chunk.words], chunk.pnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
